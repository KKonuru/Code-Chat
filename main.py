from ollama import chat, ChatResponse,embeddings
from ollama import ChatResponse
from langchain_ollama import ChatOllama
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import Language,RecursiveCharacterTextSplitter
from transformers import RobertaTokenizer, RobertaModel
import torch
import os
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.documents import Document
import numpy as np
from Embedding import CodeEmbeddingFunction
from uuid import uuid4
from langchain_core.prompts import PromptTemplate

def summarize_code(code:str,llm:ChatOllama)->str:
    template = """
        Your job is to summarize the code snippet into human language. You are going to be given React code that is either javascript,typescript, or html.
         Describe what is in the code and some relationships it may have to help vector search the code snippet.

        Code: {code}

        Answer: """
    prompt_template = PromptTemplate(
        input_variables = ["code"],
        template = template
    )
    response = llm.invoke(prompt_template.format(code=code))
    #Return the string of the response
    return response.content

def query_translation(query:str,llm:ChatOllama)->str:
    template = """
        Your job is to translate the query into text that will index the vector database. The vector database contains vectors of summaries
        generated by a llm on chunks of code. I want you to best describe the code to look for in a way that will help me find the most relevant code snippets.

        Query: {query}

        Answer: """
    prompt_template = PromptTemplate(
        input_variables = ["query"],
        template = template
    )
    response = llm.invoke(prompt_template.format(query=query))
    return response.content

def embedding_function(code)->np.ndarray:
    tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    model = RobertaModel.from_pretrained("microsoft/codebert-base", add_pooling_layer=False)
    inputs = tokenizer(code, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
    return embedding

def get_documents(dir:str,files:list[str],llm:ChatOllama) -> list[Document]:
    docs = []
    for file in files:
        lang = file.split(".")[-1]
        with open(os.path.join(dir,file),"r") as f:
            code = f.read()
        parent_dir = dir.split("/")[-1]
        if file.index("/")!=-1:
            parent_dir = file.split("/")[0]
        summary = summarize_code(code,llm)
        document = Document(
            page_content = summary,
            metadata = {"lang":Language(lang),"file":file,"parent_dir":parent_dir,"code":code}
        )
        docs.append(document)
    return docs

def splitDocs(docs:list[Document]) -> list[Document]:
    splitter_dict = {}
    split_docs = []
    for doc in docs:
        lang = doc.metadata["lang"]
        if lang not in splitter_dict:
            splitter_dict[lang] = RecursiveCharacterTextSplitter.from_language(language=lang,chunk_size=500,chunk_overlap=25)
        split_doc = splitter_dict[lang].create_documents([doc.page_content]) #Split_doc is a type list of documents
        for s in split_doc:
            s.metadata = doc.metadata
        split_docs.extend(split_doc)
    return split_docs
    
def create_vector_store(dir:str,files:str,embedding,llm:ChatOllama)->FAISS:
    #To use the splitter, we need to create Documents for the files
    chunked_docs = splitDocs(get_documents(dir,files,llm))
    
    #Create a vector store
    vector_store = FAISS(
        index = faiss.IndexFlatIP(768),
        embedding_function=embedding,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={}
    )
    uuids = [str(uuid4()) for _ in range(len(chunked_docs))]
    vector_store.add_documents(chunked_docs)

    return vector_store


def getFileNames(dir):
    files = []
    allowed_file_exts= [e.value for e in Language]
    # Loop through the files in the main directory
    for file in os.listdir(dir):
        if not file.endswith(tuple(allowed_file_exts)):
            continue
        file_path = os.path.join(dir, file)
        if os.path.isfile(file_path):
            files.append(file)
    
    # For any subdirectories, loop through them and add the file name with the path starting at dir
    for subdir, dirs, filenames in os.walk(dir):
        for file in filenames:
            if not file.endswith(tuple(allowed_file_exts)):
                continue
            # Construct the relative path
            relative_path = os.path.relpath(os.path.join(subdir, file), dir)
            files.append(relative_path)
    
    return list(set(files))

def generate(llm:ChatOllama,query:str,docs:list[Document])->str:
    prompt = """

        Your are coding assistant. You will be given a question about a codebase and will answer the questions based on the context provided to you. 
        The context is code snippets from the codebase the user is inquiring about. If there is no relevant context, you must answer with \"I do not have the necessary information to answer this question.\"
        
        Query: {query}
        
        Context: {docs_content}
        
        Answer: """
    docs_content = "\n\n".join(doc.page_content for doc in docs)
    prompt_template = PromptTemplate(
        input_variables = ["query","docs_content"],
        template = prompt
    )
    return llm.invoke(prompt_template.format(query=query,docs_content=docs_content)).content
def main():
    #Define the chat model
    llm = ChatOllama(
        model = "codellama:13b",
        temperature = 0.8,
        num_predict = 256,
        # other params ...
    )

    if torch.cuda.is_available():
        torch.set_default_device("cuda")
    elif torch.backends.mps.is_available():
        torch.set_default_device("mps")

    embedding= CodeEmbeddingFunction()
    proj_dir ="/Users/kausthubhkonuru/Personal Projects/counter-assignment" #input("Enter the path to the project directory: ")
    files =getFileNames(proj_dir)
    vector_store = create_vector_store(proj_dir,files,embedding,llm)

    while True:
        query = input("Enter your query: ")
        docs = vector_store.similarity_search(
            query_translation(query,llm),
            k=5
        )
        for res in docs:
            print("Code summary:")
            print(res.page_content)
            print("Code snippet:")
            print(res.metadata["code"])
            print("Metadata:",res.metadata["file"],",",res.metadata["parent_dir"])    
    
if __name__== "__main__":
    main()